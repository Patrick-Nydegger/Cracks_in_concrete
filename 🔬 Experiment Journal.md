# ðŸ”¬ Experiment Journal: 
# Use case: Automatic inspection of bridge piers and detection of cracks in concrete


## **Project Details**

> **Institution:** ðŸ›ï¸ FHNW School of Business
>
> **Module:** ðŸ“š Deep Learning (HS 2025)
>
> **Authors:** ðŸ‘¥ Oliver Gwerder, Patrick Nydegger
>
> **Date:** ðŸ“… October - December 2025
>
> **Weight:** âš–ï¸ 30% of the final module grade

---
## Main Objective

> To develop and evaluate a Convolutional Neural Network (CNN) capable of accurately classifying images of concrete surfaces as either "Cracked" or "Non-cracked".

### Use Case

> A drone flies along a predefined course around an object (e.g., bridge piers). It uses a camera to continuously search for cracks in the concrete. The deep learning model automatically processes these camera images in real time, possibly on the drone itself, and classifies the images as â€œpositiveâ€ or â€œnegative.â€ This means that we need a model that is as fast and small as possible, since the drone has a limited payload and only a short flight time, so speed plays a major role. As soon as an image is classified as â€œpositive,â€ the drone marks it with a spray can. The images are stored together with the GPS location. After the flight, the images are validated by a potentially more complex model and possibly further classified. The goal is to achieve a complete, seamless recording of all damage.
> For this project work, the focus was on the classification model that performs the initial assessment, possibly directly on the drone.

## Project Summary

> This project involves the entire machine learning workflow, from data analysis and preprocessing to the implementation of a baseline model and a custom-designed CNN. We will document our experiments, compare model performance using appropriate metrics, and analyze the results to determine the most effective approach for automated crack detection.

---

## ðŸ“‹ Project Checklist & Table of Contents

âœ… 1. Dataset Description and Analysis

âœ… 2. Data Splitting Strategy

âœ… 3. Choice of Evaluation Metrics

âœ… 4. Data Augmentation Strategy

âœ… 5. Choice of Loss Function

âœ… 6. Baseline Model Selection

âœ… 7. Custom Model Design

- [ ] 8. Performance Analysis
- [ ] 9. Parameter Studies & Experiments
- [ ] 10. Error Analysis (Failure Cases)
âœ… 11. (Bonus) Explainability Analysis

---

### 1. Dataset Description and Analysis

The dataset used for this project is the "Concrete Crack Images for Classification" dataset, sourced from Kaggle. 
[https://www.kaggle.com/datasets/arnavr10880/concrete-crack-images-for-classification/data]

It contains a total of 40,000 images of concrete surfaces, collected from various buildings on the METU Campus. The images are provided in RGB format with a resolution of 227x227 pixels. The dataset is generated from 458 high-resolution images (4032x3024 pixel) with the method proposed by Zhang et al (2016).
No data augmentation in terms of random rotation or flipping was applied.


#### The data is pre-labeled and divided into two distinct classes:

Positive: Images containing a visible crack.
Negative: Images of concrete surfaces without any cracks.

#### Visual analysis of sample images reveals some key characteristics:

**Image Quality:** The images vary in terms of lighting, surface texture, and coloration.
**Crack Variety:** Cracks in the 'Positive' samples range from very fine, hairline fractures to large, obvious fissures.
**Potential Challenges:** Some 'Negative' samples contain features like shadows, stains, or joints in the concrete that could visually mimic cracks. This ambiguity is expected to be the primary challenge for our classification model.


<img width="1107" height="575" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/sample_images_from_the_dataset.png" />



#### Our analysis confirms that the dataset consists of exactly 20,000 images for the 'Positive' class and 20,000 images for the 'Negative' class.
 
The dataset is perfectly balanced, with a 50/50 split between the two classes. This is an ideal scenario for a binary classification task.

#### Implications of this balance:

There is a lower risk of the model developing a bias towards a majority class, which often happens in imbalanced datasets.
Since the dataset is well-balanced, accuracy can reliably be used as a performance metric.
We do not need to employ complex techniques to handle class imbalance, such as oversampling or undersampling.
 
<img width="713" height="547" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/class_distribution_of_concrete_images.png" />



___


### 2. Data Splitting Strategy
#### Existing Split:
The original dataset from Kaggle does not provide a pre-defined training, validation, or test split. It only provides two folders, Positive and Negative, containing all 40,000 images.
#### Splitting Method:
We have partitioned the dataset into three subsets with the following ratio:

**Training Set: 70% of the data**

   -> The training set is deliberately made the largest partition. This provides the model with a rich and diverse set of 28,000 examples, which is crucial for learning the complex and subtle features that distinguish cracked from non-cracked concrete surfaces.
   
**Validation Set: 15% of the data**

   -> The validation set (6,000 images) serves as a proxy for unseen data during the training phase. It is used to monitor the model's performance epoch by epoch, allowing us to detect overfitting and to make informed decisions about hyperparameter tuning (e.g., adjusting the learning rate or deciding on the number of epochs). 
  
**Test Set: 15% of the data**

   -> The test set (6,000 images) is the final holdout set. It remains completely untouched during the entire development and tuning process. Its sole purpose is to provide a single, final, and unbiased assessment of our best model's performance on completely new data, simulating its deployment in a real-world scenario.


**Final Split:**

```
Training =      Positive: 14000 + Negative: 14000   -> Total Training:     28000

Validation =    Positive: 3000 + Negative: 3000     -> Total Validation:   6000

Test =          Positive: 3000 + Negative: 3000     -> Total Test:         6000
```

---

### 3. Choice of Evaluation Metrics
We base our evaluation of key metrics on the use case. To do this, we perform a brief risk assessment.

#### Consequences of errors:

False negative (FN â€“ genuine crack overlooked): Critical error. This means that a potentially structurally dangerous crack is not documented and remains untreated. The consequence is a high safety risk.

False positive (FP â€“ no crack is marked as a crack): Non-critical error. This only leads to an unnecessary manual recheck at this point. The consequence is higher operating costs, but no safety risk.

Our primary metric that we want to optimize is therefore **recall**.
   -> By achieving the highest possible recall, we minimize the risk of a crack not being detected and becoming a safety risk.

We choose **Specificity** as our secondary metric. This measures the model's ability to correctly identify non-cracked surfaces. A high specificity means a low number of False Positives. While less critical than Recall, a reasonably high Specificity is still desirable to keep the cost of unnecessary manual inspections low.

Further we take a look at: 
* Accuracy
* F1-Score
---

### 4. Data Augmentation Strategy
#### Necessity:
Data augmentation is essential to bridge the gap between our training data and the dynamic reality of drone-based inspection. Since the model will be deployed on a flying drone, the distance to the concrete surface will constantly fluctuate, resulting in varying image resolutions and scales. Furthermore, outdoor weather conditions introduce unpredictable changes in brightness and contrast. Additionally, cracks and defects naturally occur in arbitrary orientations. By simulating these specific variancesâ€”such as scaling, brightness adjustments, and random rotationsâ€”we ensure the model is robust enough to handle the unstable conditions of a real-world flight.

#### Selected Techniques & Justification:
##### Geometric Transformations (Simulating Physical Variations):

**RandomResizedCrop(size=224, scale=(0.8, 1.0)):** This is a powerful, compound transformation that addresses two key challenges simultaneously. By randomly cropping a region of the image (between 80% and 100% of the original area) and resizing it to 224x224, it effectively simulates:

- **Zooming:** Simulates variations in the distance between the camera and the concrete surface.
- **Shifting (Translation):** Since the crop is randomly positioned, it ensures that cracks are not always centered, forcing the model to detect them anywhere in the frame.
- **Preparation for Selected Model**: As will be evaluated later in the report, our chosen network requires an image resolution of 224x224. The excess pixels of the current resolution of 227x227 are not simply cropped, but rather the entire image is converted and scaled to the required size.
  
**RandomHorizontalFlip & RandomVerticalFlip (p=0.5):** A crack's classification is independent of its orientation. These flips teach the model this fundamental invariance.

**RandomRotation(degrees=45):** This simulates variations in camera angle, making the model robust to inspections from non-parallel viewpoints.

##### Color Transformations (Simulating Environmental Variations):

**ColorJitter(brightness=0.3, contrast=0.3):** Lighting is highly unpredictable in field inspections. Altering brightness and contrast forces the model to learn the structural shape of a crack rather than relying on specific pixel intensities, making it robust to environmental changes.



<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/visualization_of_the_on-the-fly_dataaugmentation_pipeline.png" />

---


### 5. Choice of Loss Function

The choice of loss function is being strategically managed to align the model's training objective with the critical project goal: minimizing dangerous errors.

#### 1. Baseline: Binary Cross-Entropy (BCE)
The initial model will use Binary Cross-Entropy (BCE).
BCE is the standard, mathematically robust choice for binary classification (crack vs. no crack). It will serve to establish a reliable, well-calibrated performance benchmark, ensuring the fundamental model architecture is sound before specialized adjustments are made.

However we need to understand, that the dataset's error costs are asymmetric: a False Negative (missing a crack) is catastrophically more expensive than a False Positive (a false alarm).
This necessitates optimizing the model for high Sensitivity (Recall)â€”its ability to correctly identify all actual crack cases.

#### 2. Strategic Outlook: Weighted BCE for Future Optimization
To further address this asymmetric risk in future iterations or real-world deployment, we propose implementing a Weighted Binary Cross-Entropy. This technique uses a pos_weight parameter to assign a significantly higher penalty to errors involving the critical 'Positive' (crack) class. This would directly force the training process to focus on driving down the rate of dangerous False Negatives, tailoring the model's optimization even more aggressively towards the project's paramount safety objective. For this initial study, standard BCE proved sufficient to achieve high sensitivity, but Weighted BCE remains a powerful tool for fine-tuning the safety margin. This will be added into our the parameter studies & experiments.

---

### 6. Baseline Model Selection
To select the most appropriate baseline for our "Drone Inspection" use case, we performed a comprehensive comparative analysis of four standard architectures. These models were evaluated based on their inherent suitability for safety-critical and resource-constrained environments.

| Architecture | Pros | Cons | Verdict for Baseline |
| :--- | :--- | :--- | :--- |
| **VGG-16** | Simple architecture; strong feature extraction. | Massive parameter count (~138M); very slow inference; unsuitable for drones. | **Rejected** due to inefficiency. |
| **ResNet-18** | Robust performance; standard academic baseline. | Larger (~44MB) and slower than mobile-optimized architectures. | **Rejected** in favor of higher efficiency. |
| **MobileNetV2** | **Extremely lightweight (~13MB)**; built specifically for edge devices; very fast inference; Inverted Residual Blocks efficient for gradients. | Slightly lower capacity than deep ResNets, but sufficient for binary crack detection. | **Selected** as the optimal baseline for drones. |
| **EfficientNet-B0**| State-of-the-art accuracy-to-efficiency ratio. | More complex scaling; potentially higher latency on some specific edge hardware compared to MobileNet. | **Reserve Candidate**. |

We have chosen **MobileNetV2** as our primary baseline model.

#### Architecture Overview:
- Number of convolutional layers: The architecture has an initial convolutional layer followed by 7 groups of Inverted Residual Bottleneck blocks. The feature map depth expands significantly, reaching 1280 channels in the final feature extraction layer.
- Activation functions used: ReLU6 is used throughout the network. This is a variation of ReLU capped at 6, specifically designed to remain robust when used with low-precision arithmetic on mobile devices.
- Pooling layers: The network utilizes Global Average Pooling to reduce the final spatial dimensions from 7x7 down to a 1x1 feature vector before classification.
- Regularization: Batch Normalization is applied after every internal convolution. A Dropout layer is included in the classifier block to prevent overfitting during the fine-tuning process.
- Classifier head: The original ImageNet classifier (1000 classes) was replaced with a single Linear layer (1280 input features â†’ â†’ 1 output feature) for binary classification.
 
 #### Design Justification:
Our choice is driven by the specific constraints of our **Drone Inspection Use Case**:

1.  **Efficiency is Key (Priority 2):** A drone has limited battery life and computational power. MobileNetV2 is explicitly designed for such environments. It uses **Depthwise Separable Convolutions**, which drastically reduce the number of parameters (~3.5 Million vs ~11 Million for ResNet-18) and computational cost (FLOPs) without a significant drop in accuracy for visual tasks like crack detection.
2.  **High Suitability for Edge Deployment:** Our goal is real-time or near-real-time processing on the device. MobileNetV2's low latency makes it the superior candidate for running directly on the drone's embedded hardware (e.g., Raspberry Pi or Nvidia Jetson).




#### Architecture

```
=============================================================================================================================
Layer (type:depth-idx)                             Input Shape               Output Shape              Param #
=============================================================================================================================
MobileNetV2                                        [1, 3, 224, 224]          [1, 1]                    --
â”œâ”€Sequential: 1-1                                  [1, 3, 224, 224]          [1, 1280, 7, 7]           --
â”‚    â””â”€Conv2dNormActivation: 2-1                   [1, 3, 224, 224]          [1, 32, 112, 112]         --
â”‚    â”‚    â””â”€Conv2d: 3-1                            [1, 3, 224, 224]          [1, 32, 112, 112]         864
â”‚    â”‚    â””â”€BatchNorm2d: 3-2                       [1, 32, 112, 112]         [1, 32, 112, 112]         64
â”‚    â”‚    â””â”€ReLU6: 3-3                             [1, 32, 112, 112]         [1, 32, 112, 112]         --
â”‚    â””â”€InvertedResidual: 2-2                       [1, 32, 112, 112]         [1, 16, 112, 112]         --
â”‚    â”‚    â””â”€Sequential: 3-4                        [1, 32, 112, 112]         [1, 16, 112, 112]         896
â”‚    â””â”€InvertedResidual: 2-3                       [1, 16, 112, 112]         [1, 24, 56, 56]           --
â”‚    â”‚    â””â”€Sequential: 3-5                        [1, 16, 112, 112]         [1, 24, 56, 56]           5,136
â”‚    â””â”€InvertedResidual: 2-4                       [1, 24, 56, 56]           [1, 24, 56, 56]           --
â”‚    â”‚    â””â”€Sequential: 3-6                        [1, 24, 56, 56]           [1, 24, 56, 56]           8,832
â”‚    â””â”€InvertedResidual: 2-5                       [1, 24, 56, 56]           [1, 32, 28, 28]           --
â”‚    â”‚    â””â”€Sequential: 3-7                        [1, 24, 56, 56]           [1, 32, 28, 28]           10,000
â”‚    â””â”€InvertedResidual: 2-6                       [1, 32, 28, 28]           [1, 32, 28, 28]           --
â”‚    â”‚    â””â”€Sequential: 3-8                        [1, 32, 28, 28]           [1, 32, 28, 28]           14,848
â”‚    â””â”€InvertedResidual: 2-7                       [1, 32, 28, 28]           [1, 32, 28, 28]           --
â”‚    â”‚    â””â”€Sequential: 3-9                        [1, 32, 28, 28]           [1, 32, 28, 28]           14,848
â”‚    â””â”€InvertedResidual: 2-8                       [1, 32, 28, 28]           [1, 64, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-10                       [1, 32, 28, 28]           [1, 64, 14, 14]           21,056
â”‚    â””â”€InvertedResidual: 2-9                       [1, 64, 14, 14]           [1, 64, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-11                       [1, 64, 14, 14]           [1, 64, 14, 14]           54,272
â”‚    â””â”€InvertedResidual: 2-10                      [1, 64, 14, 14]           [1, 64, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-12                       [1, 64, 14, 14]           [1, 64, 14, 14]           54,272
â”‚    â””â”€InvertedResidual: 2-11                      [1, 64, 14, 14]           [1, 64, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-13                       [1, 64, 14, 14]           [1, 64, 14, 14]           54,272
â”‚    â””â”€InvertedResidual: 2-12                      [1, 64, 14, 14]           [1, 96, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-14                       [1, 64, 14, 14]           [1, 96, 14, 14]           66,624
â”‚    â””â”€InvertedResidual: 2-13                      [1, 96, 14, 14]           [1, 96, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-15                       [1, 96, 14, 14]           [1, 96, 14, 14]           118,272
â”‚    â””â”€InvertedResidual: 2-14                      [1, 96, 14, 14]           [1, 96, 14, 14]           --
â”‚    â”‚    â””â”€Sequential: 3-16                       [1, 96, 14, 14]           [1, 96, 14, 14]           118,272
â”‚    â””â”€InvertedResidual: 2-15                      [1, 96, 14, 14]           [1, 160, 7, 7]            --
â”‚    â”‚    â””â”€Sequential: 3-17                       [1, 96, 14, 14]           [1, 160, 7, 7]            155,264
â”‚    â””â”€InvertedResidual: 2-16                      [1, 160, 7, 7]            [1, 160, 7, 7]            --
â”‚    â”‚    â””â”€Sequential: 3-18                       [1, 160, 7, 7]            [1, 160, 7, 7]            320,000
â”‚    â””â”€InvertedResidual: 2-17                      [1, 160, 7, 7]            [1, 160, 7, 7]            --
â”‚    â”‚    â””â”€Sequential: 3-19                       [1, 160, 7, 7]            [1, 160, 7, 7]            320,000
â”‚    â””â”€InvertedResidual: 2-18                      [1, 160, 7, 7]            [1, 320, 7, 7]            --
â”‚    â”‚    â””â”€Sequential: 3-20                       [1, 160, 7, 7]            [1, 320, 7, 7]            473,920
â”‚    â””â”€Conv2dNormActivation: 2-19                  [1, 320, 7, 7]            [1, 1280, 7, 7]           --
â”‚    â”‚    â””â”€Conv2d: 3-21                           [1, 320, 7, 7]            [1, 1280, 7, 7]           409,600
â”‚    â”‚    â””â”€BatchNorm2d: 3-22                      [1, 1280, 7, 7]           [1, 1280, 7, 7]           2,560
â”‚    â”‚    â””â”€ReLU6: 3-23                            [1, 1280, 7, 7]           [1, 1280, 7, 7]           --
â”œâ”€Sequential: 1-2                                  [1, 1280]                 [1, 1]                    --
â”‚    â””â”€Dropout: 2-20                               [1, 1280]                 [1, 1280]                 --
â”‚    â””â”€Linear: 2-21                                [1, 1280]                 [1, 1]                    1,281
=============================================================================================================================
Total params: 2,225,153
Trainable params: 2,225,153
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 299.53
=============================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 106.85
Params size (MB): 8.90
Estimated Total Size (MB): 116.35
=============================================================================================================================

```

<img width="1385" height="4000" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/MobileNetV2_Architecture.png" />




---



### 7. Custom Model Design

#### Architecture Overview:
- **Number of convolutional layers:** The network consists of 4 sequential convolutional blocks. The depth of the feature maps increases progressively through the network (32 â†’ â†’ 64 â†’ â†’ 128 â†’ â†’ 256 channels).
- **Activation functions used:** ReLU (Rectified Linear Unit) is used as the non-linear activation function after every Batch Normalization layer.
- **Pooling layers:** Spatial dimension reduction is handled by MaxPool2d (2x2 kernel) within each convolutional block. The feature extraction phase ends with Global Average Pooling (AdaptiveAvgPool2d), which condenses the final 14x14 feature maps into a 1x1 vector.
- **Regularization:** The model employs Batch Normalization (BatchNorm2d) after every convolution to stabilize training. Additionally, Dropout with a probability of 0.5 is applied immediately before the final classification layer to prevent overfitting.
- **Classifier head:** A minimalist design using a single Linear layer (256 input features â†’ â†’ 1 output feature) that produces the final binary logit.

#### Design Justification:
Our OPNet architecture is explicitly designed for high efficiency and deployment on resource-constrained edge devices (e.g., drones). By utilizing a Global Average Pooling layer instead of flattening the feature maps into massive fully connected layers, the model drastically reduces its parameter count to approximately 390,000. This makes it roughly 9x smaller than the MobileNetV2 baseline.
The structure follows a classic VGG-style hierarchyâ€”learning simple edges in early layers (32 filters) and complex textures in deeper layers (256 filters)â€”but removes all unnecessary architectural overhead. The inclusion of Batch Normalization ensures that despite its simplicity, the model converges quickly, while the heavy use of Dropout ensures robust generalization on the binary crack detection task.

#### Architecture

```
===================================================================================================================
Layer (type:depth-idx)                   Input Shape               Output Shape              Param #
===================================================================================================================
OPNet                                    [1, 3, 224, 224]          [1, 1]                    --
â”œâ”€Sequential: 1-1                        [1, 3, 224, 224]          [1, 32, 112, 112]         --
â”‚    â””â”€Conv2d: 2-1                       [1, 3, 224, 224]          [1, 32, 224, 224]         896
â”‚    â””â”€BatchNorm2d: 2-2                  [1, 32, 224, 224]         [1, 32, 224, 224]         64
â”‚    â””â”€ReLU: 2-3                         [1, 32, 224, 224]         [1, 32, 224, 224]         --
â”‚    â””â”€MaxPool2d: 2-4                    [1, 32, 224, 224]         [1, 32, 112, 112]         --
â”œâ”€Sequential: 1-2                        [1, 32, 112, 112]         [1, 64, 56, 56]           --
â”‚    â””â”€Conv2d: 2-5                       [1, 32, 112, 112]         [1, 64, 112, 112]         18,496
â”‚    â””â”€BatchNorm2d: 2-6                  [1, 64, 112, 112]         [1, 64, 112, 112]         128
â”‚    â””â”€ReLU: 2-7                         [1, 64, 112, 112]         [1, 64, 112, 112]         --
â”‚    â””â”€MaxPool2d: 2-8                    [1, 64, 112, 112]         [1, 64, 56, 56]           --
â”œâ”€Sequential: 1-3                        [1, 64, 56, 56]           [1, 128, 28, 28]          --
â”‚    â””â”€Conv2d: 2-9                       [1, 64, 56, 56]           [1, 128, 56, 56]          73,856
â”‚    â””â”€BatchNorm2d: 2-10                 [1, 128, 56, 56]          [1, 128, 56, 56]          256
â”‚    â””â”€ReLU: 2-11                        [1, 128, 56, 56]          [1, 128, 56, 56]          --
â”‚    â””â”€MaxPool2d: 2-12                   [1, 128, 56, 56]          [1, 128, 28, 28]          --
â”œâ”€Sequential: 1-4                        [1, 128, 28, 28]          [1, 256, 14, 14]          --
â”‚    â””â”€Conv2d: 2-13                      [1, 128, 28, 28]          [1, 256, 28, 28]          295,168
â”‚    â””â”€BatchNorm2d: 2-14                 [1, 256, 28, 28]          [1, 256, 28, 28]          512
â”‚    â””â”€ReLU: 2-15                        [1, 256, 28, 28]          [1, 256, 28, 28]          --
â”‚    â””â”€MaxPool2d: 2-16                   [1, 256, 28, 28]          [1, 256, 14, 14]          --
â”œâ”€AdaptiveAvgPool2d: 1-5                 [1, 256, 14, 14]          [1, 256, 1, 1]            --
â”œâ”€Sequential: 1-6                        [1, 256]                  [1, 1]                    --
â”‚    â””â”€Dropout: 2-17                     [1, 256]                  [1, 256]                  --
â”‚    â””â”€Linear: 2-18                      [1, 256]                  [1, 1]                    257
===================================================================================================================
Total params: 389,633
Trainable params: 389,633
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 740.00
===================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 48.17
Params size (MB): 1.56
Estimated Total Size (MB): 50.33
===================================================================================================================

```

<img width="1385" height="547" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/OPNet_Architecture.png" />

---

### 8. Performance Analysis
*   **Comparison Table:**

    | Model             | Recall    | Specificity  | Accuracy | F1-Score  |
    |-------------------|-----------|------------|----------|-----------|
    | **MobileNetV2**   |  99.90%   |   99.83%   |   99.87% |   0.9987  |
    | **OPNet**         |  99.63%   |   99.83%   |   99.73% |   0.9973  |

*   **Training Curves:**
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/baseline_training-performance.png" />

```
Created model save directory: /content/drive/MyDrive/models
Starting training for MobileNetV2...
Epoch 1/10 | Loss: 0.0085 | SENS (Recall): 0.9990 | Spec: 0.9947 | Acc: 0.9968 | F1: 0.9968
Epoch 2/10 | Loss: 0.0075 | SENS (Recall): 0.9983 | Spec: 0.9973 | Acc: 0.9978 | F1: 0.9978
Epoch 3/10 | Loss: 0.0069 | SENS (Recall): 0.9973 | Spec: 0.9980 | Acc: 0.9977 | F1: 0.9977
Epoch 4/10 | Loss: 0.0081 | SENS (Recall): 0.9977 | Spec: 0.9970 | Acc: 0.9973 | F1: 0.9973
Epoch 5/10 | Loss: 0.0071 | SENS (Recall): 0.9977 | Spec: 0.9973 | Acc: 0.9975 | F1: 0.9975
Epoch 6/10 | Loss: 0.0047 | SENS (Recall): 0.9977 | Spec: 0.9987 | Acc: 0.9982 | F1: 0.9982
Epoch 7/10 | Loss: 0.0060 | SENS (Recall): 0.9983 | Spec: 0.9973 | Acc: 0.9978 | F1: 0.9978
Epoch 8/10 | Loss: 0.0073 | SENS (Recall): 0.9973 | Spec: 0.9980 | Acc: 0.9977 | F1: 0.9977
Epoch 9/10 | Loss: 0.0085 | SENS (Recall): 0.9987 | Spec: 0.9963 | Acc: 0.9975 | F1: 0.9975
Epoch 10/10 | Loss: 0.0068 | SENS (Recall): 0.9967 | Spec: 0.9983 | Acc: 0.9975 | F1: 0.9975
MobileNetV2 complete in 29m
Starting training for OPNet...
Epoch 1/10 | Loss: 0.0371 | SENS (Recall): 0.9780 | Spec: 0.9983 | Acc: 0.9882 | F1: 0.9880
Epoch 2/10 | Loss: 0.0288 | SENS (Recall): 0.9887 | Spec: 0.9990 | Acc: 0.9938 | F1: 0.9938
Epoch 3/10 | Loss: 0.0399 | SENS (Recall): 0.9757 | Spec: 0.9973 | Acc: 0.9865 | F1: 0.9864
Epoch 4/10 | Loss: 0.0168 | SENS (Recall): 0.9960 | Spec: 0.9970 | Acc: 0.9965 | F1: 0.9965
Epoch 5/10 | Loss: 0.0163 | SENS (Recall): 0.9977 | Spec: 0.9970 | Acc: 0.9973 | F1: 0.9973
Epoch 6/10 | Loss: 0.0155 | SENS (Recall): 0.9950 | Spec: 0.9990 | Acc: 0.9970 | F1: 0.9970
Epoch 7/10 | Loss: 0.0405 | SENS (Recall): 0.9987 | Spec: 0.9833 | Acc: 0.9910 | F1: 0.9911
Epoch 8/10 | Loss: 0.0141 | SENS (Recall): 0.9937 | Spec: 0.9993 | Acc: 0.9965 | F1: 0.9965
Epoch 9/10 | Loss: 0.0121 | SENS (Recall): 0.9967 | Spec: 0.9977 | Acc: 0.9972 | F1: 0.9972
Epoch 10/10 | Loss: 0.0146 | SENS (Recall): 0.9980 | Spec: 0.9963 | Acc: 0.9972 | F1: 0.9972
OPNet complete in 27m
```  
  
*   **Interpretation:**
    *   **Baseline Superiority:** MobileNetV2 demonstrates state-of-the-art performance, achieving nearly perfect recall (99.90%) very early in training (Epoch 6). This validates the effectiveness of transfer learning from ImageNet features for texture-based tasks like crack detection.
    *   **Custom Model Efficiency:** Our custom OPNet is extremely competitive. With a recall of 99.63%, it trails the complex baseline by only 0.27%. Given that OPNet is approximately 9x smaller (~400k parameters vs. 3.5M), this result proves that a lightweight, specialized architecture is highly effective for this specific binary classification problem.
    *   **Specificity:** Both models achieved an identical, high specificity of 99.83% in their best epochs, indicating excellent resistance to false alarms.


### 9. Parameter Studies & Experiments
*   **Comparison Table:**

    | Model            |  Experiment   | Recall    | Specificity  | Accuracy | F1-Score  |
    |------------------|---------------|-----------|------------|----------|-----------|
    | **MobileNetV2**  | Baseline      |  99.90%         |    99.83%        |  99.87%        |  0.9987         |
    | **OPNet**        | Baseline      |  99.63%         |    99.83%        |  99.73%        |  0.9973         |
    | **MobileNetV2**  | Low LR (1e-4) |  99.93%   |    99.73%  |  99.83%  |  0.9983   |
    | **OPNet**        | Low LR (1e-4) |  99.73%   |    99.73%  |  99.73%  |  0.9973   |
    | **MobileNetV2**  | Small Batch (32) |  99.90%   |    99.43%  |  99.67%  |  0.9967   |
    | **OPNet**        | Small Batch (32) |  **99.83%**  |    99.70%  |  **99.77%**  |  **0.9977**   |
    | **MobileNetV2**  | Weighted Loss    | 99.90%   |   99.83%   | 99.87% |  99.87%   |
    | **OPNet**        | Weighted Loss    |  99.63%  |   99.80%   | 99.72% |  99.72%   |

    
*   **Objective:**
    The goal of these experiments was to fine-tune our custom OPNet to close the small performance gap to the baseline, specifically focusing on maximizing Recall (Sensitivity) for safety reasons.

*   **Experiment 1: Learning Rate Tuning**
    *   **Method:** We reduced the Learning Rate from $10^{-3}$ to $10^{-4}$ to allow for finer weight updates in the loss landscape.
    *   **Result:** The lower learning rate improved OPNet's Recall from 99.63% to 99.73%. The training was more stable in later epochs, avoiding oscillations. For MobileNetV2, it pushed Recall to a near-perfect 99.93%, albeit with a tiny drop in Specificity.
    *   **Conclusion:** Lower Learning Rate is beneficial for maximizing sensitivity in both architectures.

*   Comparison: MobileNetV2 LR Impact
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/experiment_MobileNetV2_Low_LR_(1e-4).png" />

```
--- Training MobileNetV2 (Low LR) ---
Starting training for MobileNetV2 (Low LR)...
Epoch 1/10 | Loss: 0.0068 | SENS (Recall): 0.9980 | Spec: 0.9980 | Acc: 0.9980 | F1: 0.9980
Epoch 2/10 | Loss: 0.0044 | SENS (Recall): 0.9993 | Spec: 0.9977 | Acc: 0.9985 | F1: 0.9985
Epoch 3/10 | Loss: 0.0039 | SENS (Recall): 0.9990 | Spec: 0.9987 | Acc: 0.9988 | F1: 0.9988
Epoch 4/10 | Loss: 0.0043 | SENS (Recall): 0.9993 | Spec: 0.9983 | Acc: 0.9988 | F1: 0.9988
Epoch 5/10 | Loss: 0.0056 | SENS (Recall): 0.9973 | Spec: 0.9993 | Acc: 0.9983 | F1: 0.9983
Epoch 6/10 | Loss: 0.0050 | SENS (Recall): 0.9980 | Spec: 0.9993 | Acc: 0.9987 | F1: 0.9987
Epoch 7/10 | Loss: 0.0060 | SENS (Recall): 0.9983 | Spec: 0.9983 | Acc: 0.9983 | F1: 0.9983
Epoch 8/10 | Loss: 0.0057 | SENS (Recall): 0.9987 | Spec: 0.9973 | Acc: 0.9980 | F1: 0.9980
Epoch 9/10 | Loss: 0.0050 | SENS (Recall): 0.9993 | Spec: 0.9973 | Acc: 0.9983 | F1: 0.9983
Epoch 10/10 | Loss: 0.0050 | SENS (Recall): 0.9990 | Spec: 0.9977 | Acc: 0.9983 | F1: 0.9983
MobileNetV2 (Low LR) complete in 28m
```

*   Comparison: OPNet LR Impact
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/experiment_OPNet_Low_LR_(1e-4).png" />

```
--- Training OPNet (Low LR) ---
Starting training for OPNet (Low LR)...
Epoch 1/10 | Loss: 0.0902 | SENS (Recall): 0.9570 | Spec: 0.9947 | Acc: 0.9758 | F1: 0.9754
Epoch 2/10 | Loss: 0.0542 | SENS (Recall): 0.9740 | Spec: 0.9960 | Acc: 0.9850 | F1: 0.9848
Epoch 3/10 | Loss: 0.0365 | SENS (Recall): 0.9847 | Spec: 0.9970 | Acc: 0.9908 | F1: 0.9908
Epoch 4/10 | Loss: 0.0282 | SENS (Recall): 0.9880 | Spec: 0.9987 | Acc: 0.9933 | F1: 0.9933
Epoch 5/10 | Loss: 0.0257 | SENS (Recall): 0.9950 | Spec: 0.9967 | Acc: 0.9958 | F1: 0.9958
Epoch 6/10 | Loss: 0.0232 | SENS (Recall): 0.9967 | Spec: 0.9980 | Acc: 0.9973 | F1: 0.9973
Epoch 7/10 | Loss: 0.0227 | SENS (Recall): 0.9917 | Spec: 0.9990 | Acc: 0.9953 | F1: 0.9953
Epoch 8/10 | Loss: 0.0304 | SENS (Recall): 0.9980 | Spec: 0.9897 | Acc: 0.9938 | F1: 0.9939
Epoch 9/10 | Loss: 0.0214 | SENS (Recall): 0.9973 | Spec: 0.9973 | Acc: 0.9973 | F1: 0.9973
Epoch 10/10 | Loss: 0.0192 | SENS (Recall): 0.9953 | Spec: 0.9987 | Acc: 0.9970 | F1: 0.9970
OPNet (Low LR) complete in 26m
```

*   **Experiment 2: Batch Size**
    *   **Method:** We reduced the Batch Size from 64 to 32. Smaller batches introduce more noise into the gradient estimation, which can act as a regularizer.
    *   **Result:** This was the most successful experiment for OPNet. It boosted Recall to **99.83%** and Accuracy to 99.77%, bringing the custom model within touching distance (0.07%) of the original baseline.
    *   **Conclusion:** The regularization effect of the smaller batch size helped the lightweight OPNet generalize better to unseen validation data.
 
*   Comparison: MobileNetV2 Batch Size Impact
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/experiment_MobileNetV2_Small_Batch_(32).png" />

```
--- Training MobileNetV2 (Batch 32) ---
Starting training for MobileNetV2 (Batch 32)...
Epoch 1/10 | Loss: 0.0102 | SENS (Recall): 0.9957 | Spec: 0.9973 | Acc: 0.9965 | F1: 0.9965
Epoch 2/10 | Loss: 0.0114 | SENS (Recall): 0.9990 | Spec: 0.9937 | Acc: 0.9963 | F1: 0.9963
Epoch 3/10 | Loss: 0.0074 | SENS (Recall): 0.9980 | Spec: 0.9957 | Acc: 0.9968 | F1: 0.9968
Epoch 4/10 | Loss: 0.0086 | SENS (Recall): 0.9967 | Spec: 0.9983 | Acc: 0.9975 | F1: 0.9975
Epoch 5/10 | Loss: 0.0105 | SENS (Recall): 0.9990 | Spec: 0.9943 | Acc: 0.9967 | F1: 0.9967
Epoch 6/10 | Loss: 0.0097 | SENS (Recall): 0.9987 | Spec: 0.9963 | Acc: 0.9975 | F1: 0.9975
Epoch 7/10 | Loss: 0.0122 | SENS (Recall): 0.9953 | Spec: 0.9993 | Acc: 0.9973 | F1: 0.9973
Epoch 8/10 | Loss: 0.0089 | SENS (Recall): 0.9973 | Spec: 0.9980 | Acc: 0.9977 | F1: 0.9977
Epoch 9/10 | Loss: 0.0060 | SENS (Recall): 0.9967 | Spec: 0.9987 | Acc: 0.9977 | F1: 0.9977
Epoch 10/10 | Loss: 0.0080 | SENS (Recall): 0.9973 | Spec: 0.9977 | Acc: 0.9975 | F1: 0.9975
MobileNetV2 (Batch 32) complete in 29m
```

*   Comparison: OPNet Batch Size Impact
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/experiment_OPNet_Small_Batch_(32).png" />

```
--- Training OPNet (Batch 32) ---
Starting training for OPNet (Batch 32)...
Epoch 1/10 | Loss: 0.0301 | SENS (Recall): 0.9873 | Spec: 0.9957 | Acc: 0.9915 | F1: 0.9915
Epoch 2/10 | Loss: 0.0634 | SENS (Recall): 0.9963 | Spec: 0.9680 | Acc: 0.9822 | F1: 0.9824
Epoch 3/10 | Loss: 0.0281 | SENS (Recall): 0.9833 | Spec: 0.9980 | Acc: 0.9907 | F1: 0.9906
Epoch 4/10 | Loss: 0.0260 | SENS (Recall): 0.9853 | Spec: 0.9983 | Acc: 0.9918 | F1: 0.9918
Epoch 5/10 | Loss: 0.0160 | SENS (Recall): 0.9920 | Spec: 0.9990 | Acc: 0.9955 | F1: 0.9955
Epoch 6/10 | Loss: 0.0144 | SENS (Recall): 0.9950 | Spec: 0.9987 | Acc: 0.9968 | F1: 0.9968
Epoch 7/10 | Loss: 0.0198 | SENS (Recall): 0.9967 | Spec: 0.9970 | Acc: 0.9968 | F1: 0.9968
Epoch 8/10 | Loss: 0.0121 | SENS (Recall): 0.9973 | Spec: 0.9980 | Acc: 0.9977 | F1: 0.9977
Epoch 9/10 | Loss: 0.0129 | SENS (Recall): 0.9983 | Spec: 0.9970 | Acc: 0.9977 | F1: 0.9977
Epoch 10/10 | Loss: 0.0163 | SENS (Recall): 0.9983 | Spec: 0.9967 | Acc: 0.9975 | F1: 0.9975
OPNet (Batch 32) complete in 26m
```



*   **Experiment 3: Weighted Loss (Sensitivity Boosting)**
    *   **Method:** We applied a pos_weight of 3.0 to the Binary Cross Entropy loss function. This penalizes the model 3x more for missing a crack (False Negative) than for a false alarm, aligning the training objective with the safety-critical nature of the task.
    *   **Result:** MobileNetV2 achieved a milestone 100.00% Recall (Sensitivity) in Epoch 7, proving it captured every single crack in the validation set, with only a minor drop in Specificity. OPNet also saw a massive boost in Recall, peaking at 99.97% (Epoch 7). However, this came with a significant trade-off: OPNet's Specificity temporarily dropped to 94.7%, indicating the lightweight model became "paranoid" and flagged non-cracks as cracks before stabilizing in later epochs.
    *   **Conclusion:** Weighted Loss is the most effective tool for ensuring zero missed defects. However, it introduces a distinct trade-off: while safety is maximized, the model generates more False Positives (false alarms), particularly in smaller architectures like OPNet.
  

<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/experiment_3_weighted_loss_v2.png" />


  ```
--- Exp 1A: MobileNetV2 (Weighted Loss) ---
Starting training for MobileNetV2 [WeightedLoss]...
Epoch 1/10 | Loss: 0.0568 | Val Loss: 0.0094 | SENS: 0.9990 | Spec: 0.9980 | Acc: 0.9985
Epoch 2/10 | Loss: 0.0298 | Val Loss: 0.0140 | SENS: 0.9990 | Spec: 0.9983 | Acc: 0.9987
Epoch 3/10 | Loss: 0.0269 | Val Loss: 0.0130 | SENS: 0.9993 | Spec: 0.9940 | Acc: 0.9967
Epoch 4/10 | Loss: 0.0210 | Val Loss: 0.0086 | SENS: 0.9993 | Spec: 0.9967 | Acc: 0.9980
Epoch 5/10 | Loss: 0.0241 | Val Loss: 0.0076 | SENS: 0.9993 | Spec: 0.9973 | Acc: 0.9983
Epoch 6/10 | Loss: 0.0233 | Val Loss: 0.0065 | SENS: 0.9993 | Spec: 0.9983 | Acc: 0.9988
Epoch 7/10 | Loss: 0.0179 | Val Loss: 0.0081 | SENS: 1.0000 | Spec: 0.9960 | Acc: 0.9980
Epoch 8/10 | Loss: 0.0202 | Val Loss: 0.0102 | SENS: 0.9997 | Spec: 0.9957 | Acc: 0.9977
Epoch 9/10 | Loss: 0.0183 | Val Loss: 0.0078 | SENS: 0.9993 | Spec: 0.9983 | Acc: 0.9988
Epoch 10/10 | Loss: 0.0209 | Val Loss: 0.0090 | SENS: 0.9990 | Spec: 0.9983 | Acc: 0.9987
MobileNetV2 training complete in 28m 32s

--- Exp 1B: OPNet (Weighted Loss) ---
Starting training for OPNet [WeightedLoss]...
Epoch 1/10 | Loss: 0.2241 | Val Loss: 0.0995 | SENS: 0.9977 | Spec: 0.9503 | Acc: 0.9740
Epoch 2/10 | Loss: 0.1034 | Val Loss: 0.0397 | SENS: 0.9957 | Spec: 0.9933 | Acc: 0.9945
Epoch 3/10 | Loss: 0.0829 | Val Loss: 0.0354 | SENS: 0.9950 | Spec: 0.9967 | Acc: 0.9958
Epoch 4/10 | Loss: 0.0726 | Val Loss: 0.0424 | SENS: 0.9913 | Spec: 0.9953 | Acc: 0.9933
Epoch 5/10 | Loss: 0.0653 | Val Loss: 0.0306 | SENS: 0.9940 | Spec: 0.9963 | Acc: 0.9952
Epoch 6/10 | Loss: 0.0593 | Val Loss: 0.0221 | SENS: 0.9977 | Spec: 0.9953 | Acc: 0.9965
Epoch 7/10 | Loss: 0.0636 | Val Loss: 0.0971 | SENS: 0.9997 | Spec: 0.9470 | Acc: 0.9733
Epoch 8/10 | Loss: 0.0469 | Val Loss: 0.0312 | SENS: 0.9987 | Spec: 0.9900 | Acc: 0.9943
Epoch 9/10 | Loss: 0.0445 | Val Loss: 0.0227 | SENS: 0.9983 | Spec: 0.9943 | Acc: 0.9963
Epoch 10/10 | Loss: 0.0488 | Val Loss: 0.0213 | SENS: 0.9963 | Spec: 0.9980 | Acc: 0.9972
OPNet training complete in 26m 45s

--- Comparison: MobileNetV2 Weighted vs. OPNet Weighted ---

--- Final Metrics (Epoch 10) for MobileNetV2 (Weighted) ---
Sensitivity: 0.9990
Specificity: 0.9983
Accuracy:    0.9987
F1-Score:    0.9987

--- Final Metrics (Epoch 10) for OPNet (Weighted) ---
Sensitivity: 0.9963
Specificity: 0.9980
Accuracy:    0.9972
F1-Score:    0.9972
```
---


### 10. Error Analysis (Failure Cases)
*   **Analysis of Misclassifications:**
    *   **False Positives (Non-Cracked predicted as Cracked):**
    *   **False Negatives (Cracked predicted as Non-Cracked):**
*   **Hypothesis:**

<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/Grad-Cam_Analysis-False_Positive.png" />
<img width="1385" height="955" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/Grad-Cam_Analysis-False_Negative.png" />

---

### 11. (Bonus) Explainability Analysis
*    **Method Used:**
To validate each model's decision-making process, we applied two complementary XAI techniques to a batch of 10 "Positive" validation images.
     * Grad-CAM (Gradient-weighted Class Activation Mapping): We visualized the activation of the final convolutional layer to identify the general regions the model focuses on.
     * Integrated Gradients (IG): We computed the pixel-level attribution by integrating gradients along a path from a black baseline image to the input image, revealing exactly which pixels contributed most to the "Crack" prediction.
*    **Findings:**
     * High Confidence: The model correctly classifies all 10 samples with near-certainty (P(Crack) â‰ˆ 1.000).
     * Localization (Grad-CAM): The Grad-CAM heatmaps (Red/Yellow blobs) consistently follow the trajectory of the cracks. Whether the crack is vertical, diagonal, or branching, the high-activation regions align perfectly with the structural defect, though the resolution is coarse.
     * Precision (Integrated Gradients): The IG visualization (Purple/Orange dots) is highly precise. It highlights the specific high-contrast edges and shadows within the cracks, ignoring the surrounding healthy concrete texture.
*   **Insights:**
The analysis confirms that the model is trustworthy. It is not relying on background artifacts or random noise to make predictions. Instead, it has successfully learned to identify the specific morphological features of a crackâ€”specifically the sharp contrast and continuous linear structure. The combination of Grad-CAM's shape detection and IG's edge sensitivity proves the model has learned a robust representation of structural defects.

#### MobileNetV2

<img width="1385" height="2000" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/XAI_MobileNetV2.png" />

#### OPNet

<img width="1385" height="2000" alt="image" src="https://github.com/Patrick-Nydegger/Cracks_in_concrete/blob/main/media/XAI_OPNet.png" />
